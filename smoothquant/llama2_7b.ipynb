{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant on Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "MODEL_PATH = \"/workspace/meta-llama/Llama-2-7b\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from smooth import smooth_lm\n",
    "from fake_quant import quantize_llama_like\n",
    "import tqdm\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))\n",
    "    \n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d1e301a9bc43f295c559ed1c0580a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_fp16 = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl_fp16 = evaluator.evaluate(model_fp16)\n",
    "# print(f\"Baseline model perplexity: {ppl_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "import numpy as np\n",
    "\n",
    "BIN_SIZE = 1000\n",
    "\n",
    "for name, m in model_fp16.model.named_modules():\n",
    "    if isinstance(m, LlamaAttention):\n",
    "        # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "        # print(m.q_proj)\n",
    "        # print(m.k_proj)\n",
    "        # print(m.v_proj)\n",
    "        # print(m.o_proj)\n",
    "\n",
    "        fig, axs = plt.subplots(3)\n",
    "        hist = torch.histogram(m.q_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "        axs[0].bar(hist[1].numpy(), hist[0].numpy(), hist[1][1]-hist[1][0])\n",
    "        axs[0].title(f'{name}.q_proj')\n",
    "        hist = torch.histogram(m.k_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "        axs[1].bar(hist[1].numpy(), hist[0].numpy(), hist[1][1]-hist[1][0])\n",
    "        axs[1].title(f'{name}.k_proj')\n",
    "        hist = torch.histogram(m.v_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "        axs[2].bar(hist[1].numpy(), hist[0].numpy(), hist[1][1]-hist[1][0])\n",
    "        axs[2].title(f'{name}.v_proj')\n",
    "\n",
    "        plt.suptitle(name)\n",
    "        # Display the figure with subplots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # if isinstance(m, LlamaMLP):\n",
    "\n",
    "    #     plt.figure(0)\n",
    "    #     hist = torch.histogram(m.gate_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "    #     plt.bar(x=hist[1].numpy()[:-1], height=hist[0].numpy())\n",
    "    #     plt.title('gate_proj')\n",
    "\n",
    "    #     # # print(m.gate_proj)\n",
    "    #     # hist = torch.histogram(m.gate_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "    #     # axs[0].bar(x=hist[1].numpy()[:-1], height=hist[0].numpy())\n",
    "    #     # axs[0].set_title('gate_proj')\n",
    "    #     # # print(m.up_proj)\n",
    "    #     # hist = torch.histogram(m.up_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "    #     # axs[1].bar(x=hist[1].numpy()[:-1], height=hist[0].numpy())\n",
    "    #     # axs[1].set_title('up_proj')\n",
    "    #     # # print(m.down_proj)\n",
    "    #     # hist = torch.histogram(m.down_proj.weight.float().cpu().detach(), bins=BIN_SIZE)\n",
    "    #     # axs[2].bar(x=hist[1].numpy()[:-1], height=hist[0].numpy())\n",
    "    #     # axs[2].set_title('down_proj')\n",
    "\n",
    "    #     # plt.suptitle(name)\n",
    "    #     # # Display the figure with subplots\n",
    "    #     # plt.tight_layout()\n",
    "    #     # plt.savefig(f'out/mlp/baseline/{name}.png')\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "import numpy as np\n",
    "\n",
    "BIN_SIZE = 1000\n",
    "\n",
    "for name, m in model_fp16.model.named_modules():\n",
    "    # if isinstance(m, LlamaAttention):\n",
    "    #     # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "    #     # print(m.q_proj)\n",
    "    #     # print(m.k_proj)\n",
    "    #     # print(m.v_proj)\n",
    "    #     # print(m.o_proj)\n",
    "    #     key_value_slicing = (m.num_key_value_heads * m.head_dim) // m.config.pretraining_tp\n",
    "    #     query_slices = m.q_proj.weight.split(\n",
    "    #         (m.num_heads * m.head_dim) // m.config.pretraining_tp, dim=0\n",
    "    #     )\n",
    "    #     key_slices = m.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "    #     value_slices = m.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "    #     fig, axs = plt.subplots(3)\n",
    "\n",
    "    #     hist = torch.histc(query_slices[0].float(), bins=1000)\n",
    "    #     axs[0].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[0].set_title('Query channel #0')\n",
    "\n",
    "    #     hist = torch.histc(key_slices[0].float(), bins=1000)\n",
    "    #     axs[1].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[1].set_title('Key channel #0')\n",
    "\n",
    "    #     hist = torch.histc(value_slices[0].float(), bins=1000)\n",
    "    #     axs[2].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[2].set_title('Value channel #0')\n",
    "\n",
    "    #     plt.suptitle(name)\n",
    "    #     # Display the figure with subplots\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    if isinstance(m, LlamaMLP):\n",
    "        fig, axs = plt.subplots(3)\n",
    "\n",
    "        # print(m.gate_proj)\n",
    "        hist = torch.histc(m.gate_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[0].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[0].set_title('gate_proj')\n",
    "        # print(m.up_proj)\n",
    "        hist = torch.histc(m.up_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[1].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[1].set_title('up_proj')\n",
    "        # print(m.down_proj)\n",
    "        hist = torch.histc(m.down_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[2].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[2].set_title('down_proj')\n",
    "\n",
    "        plt.suptitle(name)\n",
    "        # Display the figure with subplots\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'out/mlp/baseline/{name}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_fp16\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda:1\"\n",
    ")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "model_smoothquant_w8a8 = quantize_llama_like(model, weight_quant='per_tensor', act_quant='per_tensor', bits=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
    "# print(f\"SmoothQuant W8A8 quantized model perplexity: {ppl_smoothquant_w8a8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "import numpy as np\n",
    "\n",
    "BIN_SIZE = 1000\n",
    "\n",
    "for name, m in model_smoothquant_w8a8.model.named_modules():\n",
    "    # if isinstance(m, LlamaAttention):\n",
    "    #     # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "    #     # print(m.q_proj)\n",
    "    #     # print(m.k_proj)\n",
    "    #     # print(m.v_proj)\n",
    "    #     # print(m.o_proj)\n",
    "    #     key_value_slicing = (m.num_key_value_heads * m.head_dim) // m.config.pretraining_tp\n",
    "    #     query_slices = m.q_proj.weight.split(\n",
    "    #         (m.num_heads * m.head_dim) // m.config.pretraining_tp, dim=0\n",
    "    #     )\n",
    "    #     key_slices = m.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "    #     value_slices = m.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "    #     fig, axs = plt.subplots(3)\n",
    "\n",
    "    #     hist = torch.histc(query_slices[0].float(), bins=1000)\n",
    "    #     axs[0].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[0].set_title('Query channel #0')\n",
    "\n",
    "    #     hist = torch.histc(key_slices[0].float(), bins=1000)\n",
    "    #     axs[1].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[1].set_title('Key channel #0')\n",
    "\n",
    "    #     hist = torch.histc(value_slices[0].float(), bins=1000)\n",
    "    #     axs[2].bar(range(1000), hist.cpu().detach().numpy())\n",
    "    #     axs[2].set_title('Value channel #0')\n",
    "\n",
    "    #     plt.suptitle(name)\n",
    "    #     # Display the figure with subplots\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    if isinstance(m, LlamaMLP):\n",
    "        fig, axs = plt.subplots(3)\n",
    "\n",
    "        # print(m.gate_proj)\n",
    "        hist = torch.histc(m.gate_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[0].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[0].set_title('gate_proj')\n",
    "        # print(m.up_proj)\n",
    "        hist = torch.histc(m.up_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[1].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[1].set_title('up_proj')\n",
    "        # print(m.down_proj)\n",
    "        hist = torch.histc(m.down_proj.weight.float(), bins=BIN_SIZE)\n",
    "        axs[2].bar(range(BIN_SIZE), hist.cpu().detach().numpy())\n",
    "        axs[2].set_title('down_proj')\n",
    "\n",
    "        plt.suptitle(name)\n",
    "        # Display the figure with subplots\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'out/mlp/smooth_w8a8/{name}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_smoothquant_w8a8\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
