{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "MODEL_PATH = \"/workspace/meta-llama/Llama-2-7b\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from smooth import smooth_lm\n",
    "from fake_quant import quantize_llama_like\n",
    "import tqdm\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing AIQ Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda:0\")\n",
    "model_smoothquant = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda:1\")\n",
    "model_aiq = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda:2\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model_smoothquant, act_scales, 0.85)\n",
    "smooth_lm(model_aiq, act_scales, 0.85)\n",
    "\n",
    "# model_smoothquant = quantize_llama_like(model_smoothquant, weight_quant='per_channel', act_quant='per_token', bits=(6,6))\n",
    "model_smoothquant = quantize_llama_like(model_smoothquant, weight_quant='per_channel', act_quant='per_token', bits=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "val_dict = dict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, m_fp16 in model_fp16.named_modules():\n",
    "        if name in dict(model_aiq.named_modules()):\n",
    "            m_smooth = dict(model_aiq.named_modules())[name]\n",
    "\n",
    "            if isinstance(m_fp16, LlamaAttention):\n",
    "                print(f\"Module: {name}\")\n",
    "                q = (m_fp16.q_proj.weight.abs().float().cpu() - m_smooth.q_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                k = (m_fp16.k_proj.weight.abs().float().cpu() - m_smooth.k_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                v = (m_fp16.v_proj.weight.abs().float().cpu() - m_smooth.v_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                val_dict[f\"{name}.q_proj\"] = q\n",
    "                val_dict[f\"{name}.k_proj\"] = k\n",
    "                val_dict[f\"{name}.v_proj\"] = v\n",
    "            if isinstance(m_fp16, LlamaMLP):\n",
    "                print(f\"Module: {name}\")\n",
    "                g = (m_fp16.gate_proj.weight.abs().float().cpu() - m_smooth.gate_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                u = (m_fp16.up_proj.weight.abs().float().cpu() - m_smooth.up_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                d = (m_fp16.down_proj.weight.abs().float().cpu() - m_smooth.down_proj.weight.abs().float().cpu()).abs().sum(dim=-1)\n",
    "                val_dict[f\"{name}.gate_proj\"] = g\n",
    "                val_dict[f\"{name}.up_proj\"]   = u\n",
    "                val_dict[f\"{name}.down_proj\"] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_dict = dict()\n",
    "\n",
    "def create_bit_allocation_vector(x, threshold, bit):\n",
    "    import numpy as np\n",
    "\n",
    "    q = np.array(x)\n",
    "    q_min = np.min(q)\n",
    "    q_max = np.max(q)\n",
    "    q_normalized = (q - q_min) / (q_max - q_min)\n",
    "    mu, omega = q_normalized.mean(), q_normalized.std()\n",
    "    return np.where(q_normalized < mu - omega * threshold, bit-1, np.where(q_normalized >  mu + omega * threshold, bit+2, bit))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for key in val_dict:\n",
    "        bit_dict[key] = create_bit_allocation_vector(val_dict[key], 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "from fake_quant import W8A8Linear, quantize_activation_per_token_absmax\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax_map(w, map):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    scales.clamp_(min=1e-5)\n",
    "    for idx, bit in enumerate(map):\n",
    "        scales[idx] /= 2 ** (bit - 1) - 1\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "class AIQLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=6)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(AIQLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, map, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = AIQLinear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax_map(\n",
    "                module.weight, map=map # weight bits from argument\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"AIQLinear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name}, bits=W{self.bits[0]}A{self.bits[1]})\"\n",
    "\n",
    "\n",
    "def quantize_aiq(\n",
    "    model, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False):\n",
    "    from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "\n",
    "    # simulating variable bit-width integer quantization\n",
    "    for name, m in tqdm.tqdm(model.named_modules()):\n",
    "        if isinstance(m, LlamaMLP):\n",
    "            m.gate_proj = AIQLinear.from_float(\n",
    "                m.gate_proj, weight_quant=weight_quant, act_quant=act_quant, map=bit_dict[f\"{name}.gate_proj\"]\n",
    "            )\n",
    "            m.up_proj = AIQLinear.from_float(\n",
    "                m.up_proj, weight_quant=weight_quant, act_quant=act_quant, map=bit_dict[f\"{name}.up_proj\"]\n",
    "            )\n",
    "            m.down_proj = W8A8Linear.from_float(\n",
    "                m.down_proj, weight_quant=weight_quant, act_quant=act_quant, bits=(8,8)\n",
    "            )\n",
    "        elif isinstance(m, LlamaAttention):\n",
    "            m.q_proj = AIQLinear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                map=bit_dict[f\"{name}.q_proj\"]\n",
    "            )\n",
    "            m.k_proj = AIQLinear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                map=bit_dict[f\"{name}.k_proj\"]\n",
    "            )\n",
    "            m.v_proj = AIQLinear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                map=bit_dict[f\"{name}.v_proj\"]\n",
    "            )\n",
    "            m.o_proj = W8A8Linear.from_float(\n",
    "                m.o_proj, weight_quant=weight_quant, act_quant=act_quant, bits=(6,6)\n",
    "            )\n",
    "    return model\n",
    "\n",
    "model_aiq = quantize_aiq(model_aiq, weight_quant='per_channel', act_quant='per_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aiq.save_pretrained(\"llama-2-7b-aiq-w6a6\")\n",
    "# model_smoothquant.save_pretrained(\"llama-2-7b-smoothquant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "# evaluator_0 = Evaluator(dataset, tokenizer, \"cuda:0\")\n",
    "# evaluator_1 = Evaluator(dataset, tokenizer, \"cuda:1\")\n",
    "evaluator_2 = Evaluator(dataset, tokenizer, \"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_fp16 = evaluator_0.evaluate(model_fp16)\n",
    "print(f\"Perplexity of FP16 model: {ppl_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_smoothquant = evaluator_1.evaluate(model_smoothquant)\n",
    "print(f\"SmoothQuant perplexity: {ppl_smoothquant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_aiq = evaluator_2.evaluate(model_aiq)\n",
    "print(f\"AIQ perplexity: {ppl_aiq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list()\n",
    "for key in bit_dict:\n",
    "    val.append((bit_dict[key]-6).sum().item()*4096)\n",
    "import numpy as np\n",
    "np.array(val).sum()/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_aiq\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
