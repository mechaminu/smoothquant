{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "MODEL_PATH = \"/workspace/meta-llama/Llama-2-7b\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "import numpy as np\n",
    "from smooth import smooth_lm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing AIQ Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmoothQuant Activation Outlier Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_aiq = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "# smooth_lm(model_aiq, act_scales, 0.85)\n",
    "# LlamaForCausalLM.save_pretrained(model_aiq, \"llama-2-7b-smoothed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 준비\n",
    "input_text = \"Hello, I am analyzing the importance of weights and activations in the llama2 model. What I must do is to find the \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-based 중요도 계산\n",
    "absmaxs = {}\n",
    "gradient_importances = {}\n",
    "\n",
    "model_metric = LlamaForCausalLM.from_pretrained(\"llama-2-7b-smoothed\", torch_dtype=torch.float32, device_map=\"cpu\")\n",
    "model_metric.zero_grad()\n",
    "outputs = model_metric(input_ids, labels=input_ids)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "\n",
    "for name, param in tqdm(model_metric.named_parameters(), desc=\"Gradient-based Importance\"):\n",
    "    if \"layers\" in name:\n",
    "        gradient_importances[name] = param.grad.abs().mean().item()\n",
    "# for name, m in tqdm(model_metric.named_modules(), desc=\"Gradient-Based Importance\", total=len(list(model_metric.named_modules()))):\n",
    "#     if isinstance(m, LlamaAttention):\n",
    "#         # print(f\"Module: {name}\")\n",
    "#         ## TODO: head-wise split\n",
    "#         for param_name, param_value in m.named_parameters():\n",
    "#             if any(n in param_name for n in (\"q_proj\", \"k_proj\", \"v_proj\")):\n",
    "#                 # slice tensor for each head\n",
    "#                 ## (self.num_heads,(self.head_dim, self.hidden_size)) tensors\n",
    "#                 ## linear layer `{output} = {input}*{weight}^T + {bias}`.`z`\n",
    "#                 weight_slices=param_value.split(m.head_dim, dim=0)\n",
    "#                 grad_slices=param_value.grad.split(m.head_dim, dim=0)\n",
    "#                 for i, (weight, grad) in enumerate(zip(weight_slices, grad_slices)):\n",
    "#                     absmaxs[f\"{name}.{param_name}.head{i}\"] = weight.abs().max(dim=-1).values\n",
    "#                     gradient_importances[f\"{name}.{param_name}.head{i}\"] = grad.abs().mean(dim=-1)\n",
    "#             if \"o_proj\" in param_name:\n",
    "#                 head_slices=param_value.split(m.head_dim, dim=-1)\n",
    "#                 grad_slices=param_value.grad.split(m.head_dim, dim=-1)\n",
    "#                 for i, (weight, grad) in enumerate(zip(weight_slices, grad_slices)):\n",
    "#                     absmaxs[f\"{name}.{param_name}.head{i}\"] = weight.abs().max(dim=0).values  \n",
    "#                     gradient_importances[f\"{name}.{param_name}.head{i}\"] = grad.abs().mean(dim=0)\n",
    "#     if isinstance(m, LlamaMLP):\n",
    "#         for param_name, param_value in m.named_parameters():\n",
    "#             absmaxs[f\"{name}.{param_name}\"] = param_value.abs().max(dim=-1 if \"down_proj\" in param_name else 0).values\n",
    "#             gradient_importances[f\"{name}.{param_name}\"] = param_value.grad.abs().mean(dim=-1 if \"down_proj\" in param_name else 0)\n",
    "\n",
    "del model_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 민감도 기반 중요도 계산\n",
    "sensitivity_importances = {}\n",
    "\n",
    "def sensitivity_importance(model, input_ids, epsilon=1e-5):\n",
    "    model.zero_grad()\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    original_loss = outputs.loss.detach().clone()\n",
    "\n",
    "    importance_dict = {}\n",
    "    for name, param in tqdm(model.named_parameters(), desc=\"Sensitivity-based Importance\"):\n",
    "        if \"layers\" in name:\n",
    "            param_clone = param.detach().clone()\n",
    "            param.data.add_(epsilon)\n",
    "            perturbed_outputs = model(input_ids, labels=input_ids)\n",
    "            perturbed_loss = perturbed_outputs.loss\n",
    "            importance_dict[name] = (perturbed_loss - original_loss).abs().item()\n",
    "            param.data.copy_(param_clone)\n",
    "            del param_clone\n",
    "\n",
    "    return importance_dict\n",
    "\n",
    "model_metric = LlamaForCausalLM.from_pretrained(\"llama-2-7b-smoothed\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "sensitivity_importances = sensitivity_importance(model_metric, input_ids.cuda())\n",
    "\n",
    "del model_metric\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어별 중요도 계산\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "layer_outputs = {}\n",
    "model_metric = LlamaForCausalLM.from_pretrained(\"llama-2-7b-smoothed\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output_list = []\n",
    "            for o in output:\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    output_list.append(o.detach())\n",
    "                elif o is None:\n",
    "                    output_list.append(None)\n",
    "                else:\n",
    "                    output_list.append(o)  # DynamicCache 객체 등 다른 타입은 그대로 추가\n",
    "            layer_outputs[name] = tuple(output_list)\n",
    "        else:\n",
    "            layer_outputs[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for name, layer in model_metric.named_modules():\n",
    "    if isinstance(layer, (LlamaAttention, LlamaMLP)):\n",
    "        layer.register_forward_hook(get_layer_output(name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_metric(input_ids.cuda())\n",
    "\n",
    "layer_importance = {}\n",
    "for name, output in tqdm(layer_outputs.items(), desc=\"Layer-wise Importance\"):\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = None\n",
    "        for o in output:\n",
    "            if isinstance(o, torch.Tensor):\n",
    "                output_tensor = o\n",
    "                break\n",
    "        if output_tensor is not None:\n",
    "            min_size = min(output_tensor.view(-1).size(0), outputs.logits.view(-1).size(0))\n",
    "            correlation, _ = spearmanr(output_tensor.view(-1)[:min_size].cpu().numpy(), outputs.logits.view(-1)[:min_size].cpu().numpy())\n",
    "            layer_importance[name] = correlation\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        min_size = min(output.view(-1).size(0), outputs.logits.view(-1).size(0))\n",
    "        correlation, _ = spearmanr(output.view(-1)[:min_size].cpu().numpy(), outputs.logits.view(-1)[:min_size].cpu().numpy())\n",
    "        layer_importance[name] = correlation\n",
    "\n",
    "del model_metric\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Bit-Width Allocation with respect to Integrated Importance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2bits(model, target_bit, scoring_factor, lambda_factor, up_bit=2, down_bit=2):\n",
    "    scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, (LlamaAttention, LlamaMLP)):\n",
    "                Il = layer_importance[name]\n",
    "                for param_name, _ in m.named_parameters():\n",
    "                    Ig = gradient_importances[f\"{name}.{param_name}\"]\n",
    "                    Is = sensitivity_importances[f\"{name}.{param_name}\"]\n",
    "                    scores[f\"{name}.{param_name}\"] = scoring_factor.dot(np.array([Ig, Is, Il]))\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1])\n",
    "    \n",
    "    upper = [key for key, _ in sorted_scores[:int(len(sorted_scores) * lambda_factor)]]\n",
    "    lower = [key for key, _ in sorted_scores[int(len(sorted_scores) * (1 - lambda_factor)):]]\n",
    "\n",
    "    bits = dict.fromkeys(scores.keys(), target_bit)\n",
    "    for key in upper:\n",
    "        bits[key] = target_bit + up_bit\n",
    "    for key in lower:\n",
    "        bits[key] = target_bit - down_bit\n",
    "\n",
    "    return bits\n",
    "\n",
    "def quantize_aiq(\n",
    "    model, bits, activation_bit=8, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False):\n",
    "    from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "    from fake_quant import W8A8Linear\n",
    "\n",
    "    # model.cpu()\n",
    "    # model = model.float()\n",
    "\n",
    "    # from multiprocessing import Pool\n",
    "\n",
    "    # def process_module(args):\n",
    "    #     name, m, weight_quant, act_quant, bits, quantize_bmm_input = args\n",
    "    #     if isinstance(m, LlamaMLP):\n",
    "    #         for name_, m_ in m.named_modules():\n",
    "    #             if name_ in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "    #                 m_ = W8A8Linear.from_float(\n",
    "    #                     m_,\n",
    "    #                     weight_quant=weight_quant,\n",
    "    #                     act_quant=act_quant,\n",
    "    #                     bits=(bits[f\"{name}.{name_}.weight\"],8)\n",
    "    #                 )\n",
    "    #     elif isinstance(m, LlamaAttention):\n",
    "    #         for name_, m_ in m.named_modules():\n",
    "    #             if name_ in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "    #                 m_ = W8A8Linear.from_float(\n",
    "    #                     m_,\n",
    "    #                     weight_quant=weight_quant,\n",
    "    #                     act_quant=act_quant,\n",
    "    #                     quantize_output=quantize_bmm_input,\n",
    "    #                     bits=(bits[f\"{name}.{name_}.weight\"],8)\n",
    "    #                 )\n",
    "    #     return m\n",
    "\n",
    "    # from tqdm.contrib.concurrent import process_map \n",
    "\n",
    "    # with Pool() as p:\n",
    "    #     model_modules = list(model.named_modules())\n",
    "    #     args = [(name, m, weight_quant, act_quant, bits, quantize_bmm_input) for name, m in model_modules]\n",
    "    #     model_modules = process_map(process_module, args, max_workers=63)\n",
    "    \n",
    "    # model.float16()\n",
    "    # model.cuda()\n",
    "\n",
    "    # simulating variable bit-width integer quantization\n",
    "    for name, m in tqdm(model.named_modules(), total=len(list(model.named_modules()))):\n",
    "        if isinstance(m, LlamaMLP):\n",
    "            for name_, m_ in m.named_modules():\n",
    "                if name_ in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "                    m_ = W8A8Linear.from_float(\n",
    "                        m_,\n",
    "                        weight_quant=weight_quant,\n",
    "                        act_quant=act_quant,\n",
    "                        bits=(bits[f\"{name}.{name_}.weight\"],activation_bit)\n",
    "                    )\n",
    "        elif isinstance(m, LlamaAttention):\n",
    "            for name_, m_ in m.named_modules():\n",
    "                if name_ in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "                    m_ = W8A8Linear.from_float(\n",
    "                        m_,\n",
    "                        weight_quant=weight_quant,\n",
    "                        act_quant=act_quant,\n",
    "                        quantize_output=quantize_bmm_input,\n",
    "                        bits=(bits[f\"{name}.{name_}.weight\"],activation_bit)\n",
    "                    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=8, scoring_factor=np.array([1.,0.,0,]), lambda_factor=0.3)\n",
    "model = quantize_aiq(model, bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in ([1.,0.,0.,0.1], [0.,1.,0.,0.1], [0.,0.,1.,0.1], [0.5,0.5,0,0.1], [0.33,0.33,0.33,0.1]): \n",
    "    model = LlamaForCausalLM.from_pretrained(\"llama-2-7b-smoothed\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "    bits = metric2bits(model, target_bit=8, scoring_factor=np.array(elem)[:-1], lambda_factor=elem[-1])\n",
    "    model = quantize_aiq(model, bits)\n",
    "    ppl = evaluator.evaluate(model)\n",
    "    print(f\"AIQ {elem} perplexity: {ppl}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in [[0.4,0.4,0.2,x] for x in np.array([0.1,0.2,0.3,0.4,0.5])*0.2-0.01]: \n",
    "    model = LlamaForCausalLM.from_pretrained(\"llama-2-7b-smoothed\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "    bits = metric2bits(model, target_bit=8, scoring_factor=np.array(elem[:-1]), lambda_factor=elem[-1])\n",
    "    model = quantize_aiq(model, bits)\n",
    "    ppl = evaluator.evaluate(model)\n",
    "    print(f\"AIQ {elem} perplexity: {ppl}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_factor = np.array([0.4,0.4,0.2,0.1])\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=8, scoring_factor=scoring_factor[:-1], lambda_factor=scoring_factor[-1], up_bit=1, down_bit=1)\n",
    "model = quantize_aiq(model, bits)\n",
    "ppl = evaluator.evaluate(model)\n",
    "print(f\"AIQ {scoring_factor} perplexity: {ppl}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_factor = np.array([0.4,0.4,0.2,0.1])\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=4, scoring_factor=scoring_factor[:-1], lambda_factor=scoring_factor[-1], up_bit=1, down_bit=1)\n",
    "model = quantize_aiq(model, bits, activation_bit=4)\n",
    "ppl = evaluator.evaluate(model)\n",
    "print(f\"AIQ {scoring_factor} perplexity: {ppl}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list()\n",
    "for key in bits:\n",
    "    val.append((bits[key]).sum().item()*4096)\n",
    "import numpy as np\n",
    "np.array(val).sum()/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "memory_stats()\n",
    "\n",
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from fake_quant import quantize_activation_per_token_absmax\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax_map(w, bits):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=0).values\n",
    "    scales.clamp_(min=1e-5)\n",
    "\n",
    "    bits_tensor = torch.tensor(bits).cuda()\n",
    "\n",
    "    scales /= 2 ** (bits_tensor - 1) - 1\n",
    "\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "class AIQLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(AIQLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, bits, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = AIQLinear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax_map(\n",
    "                module.weight, bits=bits # weight bits from argument\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"AIQLinear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
