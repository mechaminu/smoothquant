{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "MODEL_PATH = \"/workspace/meta-llama/Llama-2-7b\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "import numpy as np\n",
    "from smooth import smooth_lm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing AIQ Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmoothQuant Activation Outlier Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_aiq = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "# smooth_lm(model_aiq, act_scales, 0.85)\n",
    "# LlamaForCausalLM.save_pretrained(model_aiq, \"llama-2-7b-smoothed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 준비\n",
    "input_text = \"Hello, I am analyzing the importance of weights and activations in the llama2 model. What I must do is to find the \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-based 중요도 계산\n",
    "absmaxs = {}\n",
    "gradient_importances = {}\n",
    "\n",
    "model_metric = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float32, device_map=\"cpu\")\n",
    "smooth_lm(model_metric, act_scales, 0.85)\n",
    "model_metric.zero_grad()\n",
    "outputs = model_metric(input_ids, labels=input_ids)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "\n",
    "for name, param in tqdm(model_metric.named_parameters(), desc=\"Gradient-based Importance\"):\n",
    "    if \"layers\" in name:\n",
    "        gradient_importances[name] = param.grad.abs().mean().item()\n",
    "## TODO: per head-channel importance 계산\n",
    "# for name, m in tqdm(model_metric.named_modules(), desc=\"Gradient-Based Importance\", total=len(list(model_metric.named_modules()))):\n",
    "#     if isinstance(m, LlamaAttention):\n",
    "#         # print(f\"Module: {name}\")\n",
    "#         for param_name, param_value in m.named_parameters():\n",
    "#             if any(n in param_name for n in (\"q_proj\", \"k_proj\", \"v_proj\")):\n",
    "#                 # slice tensor for each head\n",
    "#                 ## (self.num_heads,(self.head_dim, self.hidden_size)) tensors\n",
    "#                 ## linear layer `{output} = {input}*{weight}^T + {bias}`.`z`\n",
    "#                 weight_slices=param_value.split(m.head_dim, dim=0)\n",
    "#                 grad_slices=param_value.grad.split(m.head_dim, dim=0)\n",
    "#                 for i, (weight, grad) in enumerate(zip(weight_slices, grad_slices)):\n",
    "#                     absmaxs[f\"{name}.{param_name}.head{i}\"] = weight.abs().max(dim=-1).values\n",
    "#                     gradient_importances[f\"{name}.{param_name}.head{i}\"] = grad.abs().mean(dim=-1)\n",
    "#             if \"o_proj\" in param_name:\n",
    "#                 head_slices=param_value.split(m.head_dim, dim=-1)\n",
    "#                 grad_slices=param_value.grad.split(m.head_dim, dim=-1)\n",
    "#                 for i, (weight, grad) in enumerate(zip(weight_slices, grad_slices)):\n",
    "#                     absmaxs[f\"{name}.{param_name}.head{i}\"] = weight.abs().max(dim=0).values  \n",
    "#                     gradient_importances[f\"{name}.{param_name}.head{i}\"] = grad.abs().mean(dim=0)\n",
    "#     if isinstance(m, LlamaMLP):\n",
    "#         for param_name, param_value in m.named_parameters():\n",
    "#             absmaxs[f\"{name}.{param_name}\"] = param_value.abs().max(dim=-1 if \"down_proj\" in param_name else 0).values\n",
    "#             gradient_importances[f\"{name}.{param_name}\"] = param_value.grad.abs().mean(dim=-1 if \"down_proj\" in param_name else 0)\n",
    "\n",
    "del model_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 민감도 기반 중요도 계산\n",
    "sensitivity_importances = {}\n",
    "\n",
    "def sensitivity_importance(model, input_ids, epsilon=1e-5):\n",
    "    model.zero_grad()\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    original_loss = outputs.loss.detach().clone()\n",
    "\n",
    "    importance_dict = {}\n",
    "    for name, param in tqdm(model.named_parameters(), desc=\"Sensitivity-based Importance\"):\n",
    "        if \"layers\" in name:\n",
    "            param_clone = param.detach().clone()\n",
    "            param.data.add_(epsilon)\n",
    "            perturbed_outputs = model(input_ids, labels=input_ids)\n",
    "            perturbed_loss = perturbed_outputs.loss\n",
    "            importance_dict[name] = (perturbed_loss - original_loss).abs().item()\n",
    "            param.data.copy_(param_clone)\n",
    "            del param_clone\n",
    "\n",
    "    return importance_dict\n",
    "\n",
    "model_metric = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "smooth_lm(model_metric, act_scales, 0.85)\n",
    "sensitivity_importances = sensitivity_importance(model_metric, input_ids.cuda())\n",
    "\n",
    "del model_metric\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Metric : Layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어별 중요도 계산\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "layer_outputs = {}\n",
    "model_metric = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "smooth_lm(model_metric, act_scales, 0.85)\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output_list = []\n",
    "            for o in output:\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    output_list.append(o.detach())\n",
    "                elif o is None:\n",
    "                    output_list.append(None)\n",
    "                else:\n",
    "                    output_list.append(o)  # DynamicCache 객체 등 다른 타입은 그대로 추가\n",
    "            layer_outputs[name] = tuple(output_list)\n",
    "        else:\n",
    "            layer_outputs[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for name, layer in model_metric.named_modules():\n",
    "    if isinstance(layer, (LlamaAttention, LlamaMLP)):\n",
    "        layer.register_forward_hook(get_layer_output(name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_metric(input_ids.cuda())\n",
    "\n",
    "layer_importance = {}\n",
    "for name, output in tqdm(layer_outputs.items(), desc=\"Layer-wise Importance\"):\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = None\n",
    "        for o in output:\n",
    "            if isinstance(o, torch.Tensor):\n",
    "                output_tensor = o\n",
    "                break\n",
    "        if output_tensor is not None:\n",
    "            min_size = min(output_tensor.view(-1).size(0), outputs.logits.view(-1).size(0))\n",
    "            correlation, _ = spearmanr(output_tensor.view(-1)[:min_size].cpu().numpy(), outputs.logits.view(-1)[:min_size].cpu().numpy())\n",
    "            layer_importance[name] = correlation\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        min_size = min(output.view(-1).size(0), outputs.logits.view(-1).size(0))\n",
    "        correlation, _ = spearmanr(output.view(-1)[:min_size].cpu().numpy(), outputs.logits.view(-1)[:min_size].cpu().numpy())\n",
    "        layer_importance[name] = correlation\n",
    "\n",
    "del model_metric\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Bit-Width Allocation with respect to Integrated Importance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric2bits(model, target_bit, scoring_factor, lambda_factor, up_bit=2, down_bit=2):\n",
    "    scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, (LlamaAttention, LlamaMLP)):\n",
    "                Il = layer_importance[name]\n",
    "                for param_name, _ in m.named_parameters():\n",
    "                    Ig = gradient_importances[f\"{name}.{param_name}\"]\n",
    "                    Is = sensitivity_importances[f\"{name}.{param_name}\"]\n",
    "                    scores[f\"{name}.{param_name}\"] = scoring_factor.dot(np.array([Ig, Is, Il]))\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1])\n",
    "    \n",
    "    upper = [key for key, _ in sorted_scores[:int(len(sorted_scores) * lambda_factor)]]\n",
    "    lower = [key for key, _ in sorted_scores[int(len(sorted_scores) * (1 - lambda_factor)):]]\n",
    "\n",
    "    bits = dict.fromkeys(scores.keys(), target_bit)\n",
    "    for key in upper:\n",
    "        bits[key] = target_bit + up_bit\n",
    "    for key in lower:\n",
    "        bits[key] = target_bit - down_bit\n",
    "\n",
    "    return bits\n",
    "\n",
    "def quantize_aiq(\n",
    "    model, bits, activation_bit=8, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False):\n",
    "    from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP\n",
    "    from fake_quant import W8A8Linear\n",
    "\n",
    "    # model.cpu()\n",
    "    # model = model.float()\n",
    "\n",
    "    # from multiprocessing import Pool\n",
    "\n",
    "    # def process_module(args):\n",
    "    #     name, m, weight_quant, act_quant, bits, quantize_bmm_input = args\n",
    "    #     if isinstance(m, LlamaMLP):\n",
    "    #         for name_, m_ in m.named_modules():\n",
    "    #             if name_ in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "    #                 m_ = W8A8Linear.from_float(\n",
    "    #                     m_,\n",
    "    #                     weight_quant=weight_quant,\n",
    "    #                     act_quant=act_quant,\n",
    "    #                     bits=(bits[f\"{name}.{name_}.weight\"],8)\n",
    "    #                 )\n",
    "    #     elif isinstance(m, LlamaAttention):\n",
    "    #         for name_, m_ in m.named_modules():\n",
    "    #             if name_ in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "    #                 m_ = W8A8Linear.from_float(\n",
    "    #                     m_,\n",
    "    #                     weight_quant=weight_quant,\n",
    "    #                     act_quant=act_quant,\n",
    "    #                     quantize_output=quantize_bmm_input,\n",
    "    #                     bits=(bits[f\"{name}.{name_}.weight\"],8)\n",
    "    #                 )\n",
    "    #     return m\n",
    "\n",
    "    # from tqdm.contrib.concurrent import process_map \n",
    "\n",
    "    # with Pool() as p:\n",
    "    #     model_modules = list(model.named_modules())\n",
    "    #     args = [(name, m, weight_quant, act_quant, bits, quantize_bmm_input) for name, m in model_modules]\n",
    "    #     model_modules = process_map(process_module, args, max_workers=63)\n",
    "    \n",
    "    # model.float16()\n",
    "    # model.cuda()\n",
    "\n",
    "    # simulating variable bit-width integer quantization\n",
    "    for name, m in tqdm(model.named_modules(), total=len(list(model.named_modules()))):\n",
    "        if isinstance(m, LlamaMLP):\n",
    "            for name_, m_ in m.named_modules():\n",
    "                if name_ in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "                    m_ = W8A8Linear.from_float(\n",
    "                        m_,\n",
    "                        weight_quant=weight_quant,\n",
    "                        act_quant=act_quant,\n",
    "                        bits=(bits[f\"{name}.{name_}.weight\"],activation_bit)\n",
    "                    )\n",
    "        elif isinstance(m, LlamaAttention):\n",
    "            for name_, m_ in m.named_modules():\n",
    "                if name_ in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "                    m_ = W8A8Linear.from_float(\n",
    "                        m_,\n",
    "                        weight_quant=weight_quant,\n",
    "                        act_quant=act_quant,\n",
    "                        quantize_output=quantize_bmm_input,\n",
    "                        bits=(bits[f\"{name}.{name_}.weight\"],activation_bit)\n",
    "                    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=8, scoring_factor=np.array([1.,0.,0,]), lambda_factor=0.3)\n",
    "model = quantize_aiq(model, bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "data_files = {\"validation\": \"en/c4-validation.*.json.gz\"}\n",
    "dataset = load_dataset('../dataset/c4', data_files=data_files, split=\"validation[:1%]\")\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17365bba3e024fda90e54091e3e96471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516ce1963b774487a813235fbbdfcfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ff1bb505cc48eda95345434ca242a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1], [0.0, 0.0, 1.0, 0.1], [0.5, 0.5, 0, 0.1], [0.33, 0.33, 0.33, 0.1]] perplexity: 7.346879482269287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c706d6cbb04c92b68e7bb4ff44640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d75d27b2a144c4d9d2541dfbce98d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6412be3f7fac40ab95fc87355ab79fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1], [0.0, 0.0, 1.0, 0.1], [0.5, 0.5, 0, 0.1], [0.33, 0.33, 0.33, 0.1]] perplexity: 7.353400230407715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafdea1f51484125b1829a693f17ab5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c415d53020d411c9fed654376278ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379a7293165343aa81cd32859537e8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1], [0.0, 0.0, 1.0, 0.1], [0.5, 0.5, 0, 0.1], [0.33, 0.33, 0.33, 0.1]] perplexity: 7.323288917541504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a66ff795a204601860d8079148051e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a839e674154f55806dceb71dbfa4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2f285790f94b1bb9579a67a274d055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1], [0.0, 0.0, 1.0, 0.1], [0.5, 0.5, 0, 0.1], [0.33, 0.33, 0.33, 0.1]] perplexity: 7.34963846206665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762a97dfbfa840f38b78e21323f87faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84eb153c75848f99fdaf11c6c69dafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7126fabe10414b80d3e98cb373f05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1], [0.0, 0.0, 1.0, 0.1], [0.5, 0.5, 0, 0.1], [0.33, 0.33, 0.33, 0.1]] perplexity: 7.323629856109619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114d97b5c50c4780ada93a4218d7a900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc227a0feac492a9a6c608d1a09baed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57274bd91cee4c53810e78d69111ccb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.3], [0.0, 1.0, 0.0, 0.3], [0.0, 0.0, 1.0, 0.3], [0.5, 0.5, 0, 0.3], [0.33, 0.33, 0.33, 0.3]] perplexity: 7.353277683258057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c67f2f55b44835973296eab06badbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafdd3fbfc134a31b9d48b4e700330b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d24f62a03654d6fbea5dfcf56f7ada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.3], [0.0, 1.0, 0.0, 0.3], [0.0, 0.0, 1.0, 0.3], [0.5, 0.5, 0, 0.3], [0.33, 0.33, 0.33, 0.3]] perplexity: 7.376354694366455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf5817d97614cccb24d9d629f279c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bfaee92934465d836b1c9a6802dc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0354095deca426084658a1c790e9107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.3], [0.0, 1.0, 0.0, 0.3], [0.0, 0.0, 1.0, 0.3], [0.5, 0.5, 0, 0.3], [0.33, 0.33, 0.33, 0.3]] perplexity: 7.3580756187438965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb0c6bdffe41f2aef419b00dec3a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda2b0853ab54c1f8958035a30d3b8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096bf028d1044ace887c1540728d5105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.3], [0.0, 1.0, 0.0, 0.3], [0.0, 0.0, 1.0, 0.3], [0.5, 0.5, 0, 0.3], [0.33, 0.33, 0.33, 0.3]] perplexity: 7.376228332519531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7445599770874caa82a75da6937076df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29713cfaca7d48e898a393f2adba4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701615241f224747b33ae0b61450031a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [[1.0, 0.0, 0.0, 0.3], [0.0, 1.0, 0.0, 0.3], [0.0, 0.0, 1.0, 0.3], [0.5, 0.5, 0, 0.3], [0.33, 0.33, 0.33, 0.3]] perplexity: 7.360222816467285\n"
     ]
    }
   ],
   "source": [
    "for elem in [[[1.,0.,0.,x], [0.,1.,0.,x], [0.,0.,1.,x], [0.5,0.5,0,x], [0.33,0.33,0.33,x]] for x in [0.1, 0.3]]:\n",
    "    for elem2 in elem:\n",
    "        model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "        act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "        smooth_lm(model, act_scales, 0.85)\n",
    "        bits = metric2bits(model, target_bit=8, scoring_factor=np.array(elem2)[:-1], lambda_factor=elem2[-1])\n",
    "        model = quantize_aiq(model, bits)\n",
    "        ppl = evaluator.evaluate(model)\n",
    "        print(f\"AIQ {elem} perplexity: {ppl}\")\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8784242afa0049959660047c499f832e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9948991f3f4842faa5c790837c4c5a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010b4e08353143aab2b31680d7f9583b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.01] perplexity: 7.306535720825195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1b7b8330684c1f8618337d32130faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f78466a1bb74d9cb59f911131da07c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e6c1ffc3524f3ca9e13dc733f0fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.03] perplexity: 7.308714866638184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64d4cd218af49fd93ea61f460b917ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e3327331974fa4803d25b096eb9c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffe8d9c23c7404089e7b84130128385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.05] perplexity: 7.309116363525391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dedf29645a4490a980b4f06eecae99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c99c78d9bbc4505a3c4052805c4917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abef89a01cbf4beca50d283c0e123a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.07] perplexity: 7.318541526794434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba47ca892f7045d4b34981b779289f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2597158fed474e61bd409d98456c9809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644d4d4107794daa91a872e9d24ed39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.09] perplexity: 7.321682929992676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ed81249724b05ada35c7b7932df91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80695e94b6e84ebaab20aa1bc73f8586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede536d4fe604fb9b67324fa5e8c9f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.1] perplexity: 7.323244571685791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67cebad2cb24c9e9db06aa66ff540b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3f2f2f26ae475b8c73c6063ecd90ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533502bb05de47498b11aa049b5663ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.2] perplexity: 7.338731288909912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9627fd97fe4ae9a0a146025c07e1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88874737fa3a46cdbdc907163b7eb955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18955b5d623e47f9a5df8904014953a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.3] perplexity: 7.359064102172852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be711a605d8046cda7f03c1076c78d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42936786e5684c4da59f1943faae7ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3f5246ff7448458ce9904947e83c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.4] perplexity: 7.363735675811768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a742867aca3d42c4a5ff7a480a8be54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e4bec8efd348989f6f37e424e1c665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fb12dd6fa141c482f2d0b4c1361725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIQ [0.4, 0.4, 0.2, 0.5] perplexity: 7.380841255187988\n"
     ]
    }
   ],
   "source": [
    "for elem in [[0.4,0.4,0.2,x] for x in np.round(np.concatenate((np.array([0.1,0.2,0.3,0.4,0.5])*0.2-0.01,np.array([1,2,3,4,5])/10),axis=0),2)]: \n",
    "    model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "    act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "    smooth_lm(model, act_scales, 0.85)\n",
    "    bits = metric2bits(model, target_bit=8, scoring_factor=np.array(elem[:-1]), lambda_factor=elem[-1])\n",
    "    model = quantize_aiq(model, bits)\n",
    "    ppl = evaluator.evaluate(model)\n",
    "    print(f\"AIQ {elem} perplexity: {ppl}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_factor = np.array([0.4,0.4,0.2,0.03])\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=8, scoring_factor=scoring_factor[:-1], lambda_factor=scoring_factor[-1], up_bit=1, down_bit=1)\n",
    "model = quantize_aiq(model, bits)\n",
    "ppl = evaluator.evaluate(model)\n",
    "print(f\"AIQ {scoring_factor} perplexity: {ppl}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_factor = np.array([0.4,0.4,0.2,0.03])\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=4, scoring_factor=scoring_factor[:-1], lambda_factor=scoring_factor[-1], up_bit=1, down_bit=1)\n",
    "model = quantize_aiq(model, bits, activation_bit=4)\n",
    "ppl = evaluator.evaluate(model)\n",
    "print(f\"AIQ {scoring_factor} perplexity: {ppl}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e463ab094c4bab91e618a4bdc6e6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3669b7a942ff4bccb5bf3e760d354f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05:10:58:04,844 WARNING  [huggingface.py:118] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-06-05:10:58:04,925 WARNING  [huggingface.py:337] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "scoring_factor = np.array([0.4,0.4,0.2,0.03])\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm(model, act_scales, 0.85)\n",
    "bits = metric2bits(model, target_bit=4, scoring_factor=scoring_factor[:-1], lambda_factor=scoring_factor[-1], up_bit=1, down_bit=1)\n",
    "model = quantize_aiq(model, bits, activation_bit=4)\n",
    "\n",
    "lm_obj = HFLM(pretrained=model, tokenizer=tokenizer, dtype=torch.float16, batch_size=8)\n",
    "\n",
    "# indexes all tasks from the `lm_eval/tasks` subdirectory.\n",
    "# Alternatively, you can set `TaskManager(include_path=\"path/to/my/custom/task/configs\")`\n",
    "# to include a set of tasks in a separate directory.\n",
    "task_manager = lm_eval.tasks.TaskManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05:10:58:08,913 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/miniconda3/envs/lm-eval/lib/python3.8/site-packages/datasets/load.py:1491: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-06-05:10:58:16,513 WARNING  [evaluator.py:222] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-06-05:10:58:16,516 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 4186.13it/s]\n",
      "2024-06-05:10:58:20,071 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 40168/40168 [16:14<00:00, 41.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |0.2872|±  |0.0045|\n",
      "|         |       |none  |     0|acc_norm|0.3269|±  |0.0047|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting `task_manager` to the one above is optional and should generally be done\n",
    "# if you want to include tasks from paths other than ones in `lm_eval/tasks`.\n",
    "# `simple_evaluate` will instantiate its own task_manager is the it is set to None here.\n",
    "results = lm_eval.simple_evaluate( # call simple_evaluate\n",
    "    model=lm_obj,\n",
    "    tasks=[\"hellaswag\"],\n",
    "    num_fewshot=0,\n",
    "    # limit=0.1,\n",
    "    task_manager=task_manager\n",
    ")\n",
    "\n",
    "print(lm_eval.utils.make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05:11:18:30,698 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "/root/miniconda3/envs/lm-eval/lib/python3.8/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-06-05:11:20:45,246 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "2024-06-05:11:20:45,248 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "2024-06-05:11:20:45,248 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "2024-06-05:11:20:45,249 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "2024-06-05:11:20:45,249 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "2024-06-05:11:20:45,250 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "2024-06-05:11:20:45,250 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "2024-06-05:11:20:45,250 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "2024-06-05:11:20:45,251 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "2024-06-05:11:20:45,251 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "2024-06-05:11:20:45,251 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "2024-06-05:11:20:45,252 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "2024-06-05:11:20:45,252 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "2024-06-05:11:20:45,252 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "2024-06-05:11:20:45,253 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "2024-06-05:11:20:45,253 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "2024-06-05:11:20:45,253 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "2024-06-05:11:20:45,254 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "2024-06-05:11:20:45,254 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "2024-06-05:11:20:45,254 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "2024-06-05:11:20:45,255 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "2024-06-05:11:20:45,255 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "2024-06-05:11:20:45,256 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "2024-06-05:11:20:45,257 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "2024-06-05:11:20:45,257 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "2024-06-05:11:20:45,257 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "2024-06-05:11:20:45,258 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "2024-06-05:11:20:45,258 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "2024-06-05:11:20:45,258 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "2024-06-05:11:20:45,259 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "2024-06-05:11:20:45,259 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "2024-06-05:11:20:45,260 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "2024-06-05:11:20:45,260 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "2024-06-05:11:20:45,260 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "2024-06-05:11:20:45,261 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "2024-06-05:11:20:45,261 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "2024-06-05:11:20:45,261 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "2024-06-05:11:20:45,261 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "2024-06-05:11:20:45,262 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "2024-06-05:11:20:45,263 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "2024-06-05:11:20:45,263 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "2024-06-05:11:20:45,263 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "2024-06-05:11:20:45,264 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "2024-06-05:11:20:45,264 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "2024-06-05:11:20:45,264 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "2024-06-05:11:20:45,265 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "2024-06-05:11:20:45,265 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "2024-06-05:11:20:45,265 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "2024-06-05:11:20:45,266 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "2024-06-05:11:20:45,266 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "2024-06-05:11:20:45,266 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "2024-06-05:11:20:45,267 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "2024-06-05:11:20:45,268 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "2024-06-05:11:20:45,268 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "2024-06-05:11:20:45,269 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "2024-06-05:11:20:45,269 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "2024-06-05:11:20:45,269 WARNING  [evaluator.py:222] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "2024-06-05:11:20:45,280 INFO     [task.py:395] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 487.28it/s]\n",
      "2024-06-05:11:20:45,328 INFO     [task.py:395] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 977.91it/s]\n",
      "2024-06-05:11:20:45,352 INFO     [task.py:395] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 954.79it/s]\n",
      "2024-06-05:11:20:45,376 INFO     [task.py:395] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 911.47it/s]\n",
      "2024-06-05:11:20:45,402 INFO     [task.py:395] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 954.62it/s]\n",
      "2024-06-05:11:20:45,427 INFO     [task.py:395] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 958.29it/s]\n",
      "2024-06-05:11:20:45,451 INFO     [task.py:395] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 911.48it/s]\n",
      "2024-06-05:11:20:45,477 INFO     [task.py:395] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 946.06it/s]\n",
      "2024-06-05:11:20:45,501 INFO     [task.py:395] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 902.96it/s]\n",
      "2024-06-05:11:20:45,527 INFO     [task.py:395] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 944.10it/s]\n",
      "2024-06-05:11:20:45,552 INFO     [task.py:395] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 947.17it/s]\n",
      "2024-06-05:11:20:45,577 INFO     [task.py:395] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 971.72it/s]\n",
      "2024-06-05:11:20:45,601 INFO     [task.py:395] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 953.78it/s]\n",
      "2024-06-05:11:20:45,626 INFO     [task.py:395] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 899.78it/s]\n",
      "2024-06-05:11:20:45,652 INFO     [task.py:395] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 962.33it/s]\n",
      "2024-06-05:11:20:45,676 INFO     [task.py:395] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 947.08it/s]\n",
      "2024-06-05:11:20:45,701 INFO     [task.py:395] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 945.17it/s]\n",
      "2024-06-05:11:20:45,725 INFO     [task.py:395] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 927.31it/s]\n",
      "2024-06-05:11:20:45,751 INFO     [task.py:395] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 959.27it/s]\n",
      "2024-06-05:11:20:45,776 INFO     [task.py:395] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 929.99it/s]\n",
      "2024-06-05:11:20:45,801 INFO     [task.py:395] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 956.07it/s]\n",
      "2024-06-05:11:20:45,826 INFO     [task.py:395] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 939.51it/s]\n",
      "2024-06-05:11:20:45,851 INFO     [task.py:395] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 914.60it/s]\n",
      "2024-06-05:11:20:45,876 INFO     [task.py:395] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 983.83it/s]\n",
      "2024-06-05:11:20:45,900 INFO     [task.py:395] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 971.75it/s]\n",
      "2024-06-05:11:20:45,925 INFO     [task.py:395] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 914.82it/s]\n",
      "2024-06-05:11:20:45,950 INFO     [task.py:395] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 961.85it/s]\n",
      "2024-06-05:11:20:45,975 INFO     [task.py:395] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 952.67it/s]\n",
      "2024-06-05:11:20:46,000 INFO     [task.py:395] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 925.63it/s]\n",
      "2024-06-05:11:20:46,025 INFO     [task.py:395] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 961.22it/s]\n",
      "2024-06-05:11:20:46,049 INFO     [task.py:395] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 964.18it/s]\n",
      "2024-06-05:11:20:46,074 INFO     [task.py:395] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 867.29it/s]\n",
      "2024-06-05:11:20:46,101 INFO     [task.py:395] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 957.36it/s]\n",
      "2024-06-05:11:20:46,126 INFO     [task.py:395] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 956.20it/s]\n",
      "2024-06-05:11:20:46,150 INFO     [task.py:395] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 938.09it/s]\n",
      "2024-06-05:11:20:46,176 INFO     [task.py:395] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 950.15it/s]\n",
      "2024-06-05:11:20:46,200 INFO     [task.py:395] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 971.05it/s]\n",
      "2024-06-05:11:20:46,225 INFO     [task.py:395] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 943.15it/s]\n",
      "2024-06-05:11:20:46,250 INFO     [task.py:395] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 932.44it/s]\n",
      "2024-06-05:11:20:46,275 INFO     [task.py:395] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 962.15it/s]\n",
      "2024-06-05:11:20:46,299 INFO     [task.py:395] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 922.58it/s]\n",
      "2024-06-05:11:20:46,325 INFO     [task.py:395] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 951.63it/s]\n",
      "2024-06-05:11:20:46,350 INFO     [task.py:395] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 956.78it/s]\n",
      "2024-06-05:11:20:46,375 INFO     [task.py:395] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 947.69it/s]\n",
      "2024-06-05:11:20:46,399 INFO     [task.py:395] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 936.93it/s]\n",
      "2024-06-05:11:20:46,425 INFO     [task.py:395] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 955.02it/s]\n",
      "2024-06-05:11:20:46,449 INFO     [task.py:395] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 978.16it/s]\n",
      "2024-06-05:11:20:46,474 INFO     [task.py:395] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 931.46it/s]\n",
      "2024-06-05:11:20:46,499 INFO     [task.py:395] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 962.14it/s]\n",
      "2024-06-05:11:20:46,524 INFO     [task.py:395] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 942.95it/s]\n",
      "2024-06-05:11:20:46,549 INFO     [task.py:395] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 939.06it/s]\n",
      "2024-06-05:11:20:46,574 INFO     [task.py:395] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 942.97it/s]\n",
      "2024-06-05:11:20:46,599 INFO     [task.py:395] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 961.26it/s]\n",
      "2024-06-05:11:20:46,624 INFO     [task.py:395] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 941.23it/s]\n",
      "2024-06-05:11:20:46,649 INFO     [task.py:395] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 931.28it/s]\n",
      "2024-06-05:11:20:46,674 INFO     [task.py:395] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 961.17it/s]\n",
      "2024-06-05:11:20:46,699 INFO     [task.py:395] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 21/21 [00:00<00:00, 950.56it/s]\n",
      "2024-06-05:11:20:46,723 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 4788/4788 [00:36<00:00, 131.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|\n",
      "|mmlu                                   |N/A    |none  |     0|acc   |0.2331|±  |0.0122|\n",
      "| - humanities                          |N/A    |none  |     0|acc   |0.2454|±  |0.0264|\n",
      "|  - formal_logic                       |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - high_school_european_history       |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - high_school_us_history             |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - high_school_world_history          |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - international_law                  |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - jurisprudence                      |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - logical_fallacies                  |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - moral_disputes                     |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - moral_scenarios                    |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - philosophy                         |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - prehistory                         |      0|none  |     0|acc   |0.3333|±  |0.1054|\n",
      "|  - professional_law                   |      0|none  |     0|acc   |0.3333|±  |0.1054|\n",
      "|  - world_religions                    |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "| - other                               |N/A    |none  |     0|acc   |0.2308|±  |0.0257|\n",
      "|  - business_ethics                    |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - clinical_knowledge                 |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - college_medicine                   |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - global_facts                       |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - human_aging                        |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - management                         |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - marketing                          |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - medical_genetics                   |      0|none  |     0|acc   |0.3810|±  |0.1086|\n",
      "|  - miscellaneous                      |      0|none  |     0|acc   |0.3810|±  |0.1086|\n",
      "|  - nutrition                          |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - professional_accounting            |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - professional_medicine              |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - virology                           |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "| - social_sciences                     |N/A    |none  |     0|acc   |0.2063|±  |0.0248|\n",
      "|  - econometrics                       |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - high_school_geography              |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - high_school_government_and_politics|      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - high_school_macroeconomics         |      0|none  |     0|acc   |0.0952|±  |0.0656|\n",
      "|  - high_school_microeconomics         |      0|none  |     0|acc   |0.0476|±  |0.0476|\n",
      "|  - high_school_psychology             |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - human_sexuality                    |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - professional_psychology            |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - public_relations                   |      0|none  |     0|acc   |0.0476|±  |0.0476|\n",
      "|  - security_studies                   |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - sociology                          |      0|none  |     0|acc   |0.5238|±  |0.1117|\n",
      "|  - us_foreign_policy                  |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "| - stem                                |N/A    |none  |     0|acc   |0.2431|±  |0.0216|\n",
      "|  - abstract_algebra                   |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - anatomy                            |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - astronomy                          |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - college_biology                    |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - college_chemistry                  |      0|none  |     0|acc   |0.1429|±  |0.0782|\n",
      "|  - college_computer_science           |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - college_mathematics                |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - college_physics                    |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - computer_security                  |      0|none  |     0|acc   |0.3333|±  |0.1054|\n",
      "|  - conceptual_physics                 |      0|none  |     0|acc   |0.3333|±  |0.1054|\n",
      "|  - electrical_engineering             |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - elementary_mathematics             |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - high_school_biology                |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - high_school_chemistry              |      0|none  |     0|acc   |0.4286|±  |0.1107|\n",
      "|  - high_school_computer_science       |      0|none  |     0|acc   |0.2381|±  |0.0952|\n",
      "|  - high_school_mathematics            |      0|none  |     0|acc   |0.1905|±  |0.0878|\n",
      "|  - high_school_physics                |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "|  - high_school_statistics             |      0|none  |     0|acc   |0.0476|±  |0.0476|\n",
      "|  - machine_learning                   |      0|none  |     0|acc   |0.2857|±  |0.1010|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting `task_manager` to the one above is optional and should generally be done\n",
    "# if you want to include tasks from paths other than ones in `lm_eval/tasks`.\n",
    "# `simple_evaluate` will instantiate its own task_manager is the it is set to None here.\n",
    "results = lm_eval.simple_evaluate( # call simple_evaluate\n",
    "    model=lm_obj,\n",
    "    tasks=[\"mmlu\"],\n",
    "    num_fewshot=0,\n",
    "    limit=0.1,\n",
    "    task_manager=task_manager\n",
    ")\n",
    "\n",
    "print(lm_eval.utils.make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list()\n",
    "for key in bits:\n",
    "    val.append((bits[key]).sum().item()*4096)\n",
    "import numpy as np\n",
    "np.array(val).sum()/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "memory_stats()\n",
    "\n",
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from fake_quant import quantize_activation_per_token_absmax\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax_map(w, bits):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=0).values\n",
    "    scales.clamp_(min=1e-5)\n",
    "\n",
    "    bits_tensor = torch.tensor(bits).cuda()\n",
    "\n",
    "    scales /= 2 ** (bits_tensor - 1) - 1\n",
    "\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "class AIQLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(AIQLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, bits, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = AIQLinear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax_map(\n",
    "                module.weight, bits=bits # weight bits from argument\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"AIQLinear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
